{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
      "Extracting C:\\Users\\ivano\\Desktop\\DSMMaster\\data\\PROTEINS\\PROTEINS\\PROTEINS.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import DenseGraphConv, GCNConv, dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath('C:\\\\Users\\\\ivano\\\\Desktop\\\\DSMMaster\\\\CryptoThesis\\\\data')), '..', 'data', 'PROTEINS')\n",
    "dataset = TUDataset(path, name='PROTEINS').shuffle()\n",
    "average_nodes = int(dataset.data.x.size(0) / len(dataset))\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(average_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        num_nodes = ceil(0.5 * average_nodes)\n",
    "        self.pool1 = Linear(hidden_channels, num_nodes)\n",
    "\n",
    "        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
    "        num_nodes = ceil(0.5 * num_nodes)\n",
    "        self.pool2 = Linear(hidden_channels, num_nodes)\n",
    "\n",
    "        self.conv3 = DenseGraphConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "\n",
    "        s = self.pool1(x)\n",
    "        x, adj, mc1, o1 = dense_mincut_pool(x, adj, s, mask)\n",
    "\n",
    "        x = self.conv2(x, adj).relu()\n",
    "        s = self.pool2(x)\n",
    "\n",
    "        x, adj, mc2, o2 = dense_mincut_pool(x, adj, s)\n",
    "\n",
    "        x = self.conv3(x, adj)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), mc1 + mc2, o1 + o2\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.058, Train Acc: 0.601, Val Loss: 1.062, Val Acc: 0.580, Test Loss: 1.074, Test Acc: 0.571\n",
      "Epoch: 002, Train Loss: 1.010, Train Acc: 0.729, Val Loss: 1.048, Val Acc: 0.670, Test Loss: 1.061, Test Acc: 0.661\n",
      "Epoch: 003, Train Loss: 0.983, Train Acc: 0.750, Val Loss: 1.035, Val Acc: 0.679, Test Loss: 1.050, Test Acc: 0.679\n",
      "Epoch: 004, Train Loss: 0.964, Train Acc: 0.750, Val Loss: 1.027, Val Acc: 0.670, Test Loss: 1.049, Test Acc: 0.688\n",
      "Epoch: 005, Train Loss: 0.952, Train Acc: 0.757, Val Loss: 1.020, Val Acc: 0.670, Test Loss: 1.043, Test Acc: 0.688\n",
      "Epoch: 006, Train Loss: 0.941, Train Acc: 0.760, Val Loss: 1.013, Val Acc: 0.670, Test Loss: 1.039, Test Acc: 0.679\n",
      "Epoch: 007, Train Loss: 0.931, Train Acc: 0.759, Val Loss: 1.009, Val Acc: 0.661, Test Loss: 1.037, Test Acc: 0.688\n",
      "Epoch: 008, Train Loss: 0.923, Train Acc: 0.768, Val Loss: 1.004, Val Acc: 0.688, Test Loss: 1.037, Test Acc: 0.696\n",
      "Epoch: 009, Train Loss: 0.916, Train Acc: 0.773, Val Loss: 0.999, Val Acc: 0.679, Test Loss: 1.038, Test Acc: 0.679\n",
      "Epoch: 010, Train Loss: 0.913, Train Acc: 0.776, Val Loss: 0.994, Val Acc: 0.670, Test Loss: 1.036, Test Acc: 0.679\n",
      "Epoch: 011, Train Loss: 0.911, Train Acc: 0.775, Val Loss: 0.990, Val Acc: 0.723, Test Loss: 1.031, Test Acc: 0.679\n",
      "Epoch: 012, Train Loss: 0.909, Train Acc: 0.773, Val Loss: 0.985, Val Acc: 0.723, Test Loss: 1.023, Test Acc: 0.679\n",
      "Epoch: 013, Train Loss: 0.906, Train Acc: 0.774, Val Loss: 0.981, Val Acc: 0.732, Test Loss: 1.018, Test Acc: 0.688\n",
      "Epoch: 014, Train Loss: 0.903, Train Acc: 0.773, Val Loss: 0.979, Val Acc: 0.732, Test Loss: 1.015, Test Acc: 0.670\n",
      "Epoch: 015, Train Loss: 0.900, Train Acc: 0.772, Val Loss: 0.976, Val Acc: 0.723, Test Loss: 1.012, Test Acc: 0.670\n",
      "Epoch: 016, Train Loss: 0.898, Train Acc: 0.773, Val Loss: 0.975, Val Acc: 0.714, Test Loss: 1.012, Test Acc: 0.670\n",
      "Epoch: 017, Train Loss: 0.897, Train Acc: 0.775, Val Loss: 0.974, Val Acc: 0.714, Test Loss: 1.010, Test Acc: 0.670\n",
      "Epoch: 018, Train Loss: 0.896, Train Acc: 0.777, Val Loss: 0.973, Val Acc: 0.714, Test Loss: 1.009, Test Acc: 0.670\n",
      "Epoch: 019, Train Loss: 0.895, Train Acc: 0.775, Val Loss: 0.973, Val Acc: 0.714, Test Loss: 1.007, Test Acc: 0.670\n",
      "Epoch: 020, Train Loss: 0.895, Train Acc: 0.775, Val Loss: 0.972, Val Acc: 0.714, Test Loss: 1.007, Test Acc: 0.670\n",
      "Epoch: 021, Train Loss: 0.894, Train Acc: 0.774, Val Loss: 0.972, Val Acc: 0.714, Test Loss: 1.005, Test Acc: 0.670\n",
      "Epoch: 022, Train Loss: 0.893, Train Acc: 0.776, Val Loss: 0.971, Val Acc: 0.714, Test Loss: 1.004, Test Acc: 0.670\n",
      "Epoch: 023, Train Loss: 0.893, Train Acc: 0.776, Val Loss: 0.971, Val Acc: 0.714, Test Loss: 1.004, Test Acc: 0.679\n",
      "Epoch: 024, Train Loss: 0.893, Train Acc: 0.777, Val Loss: 0.971, Val Acc: 0.705, Test Loss: 1.004, Test Acc: 0.688\n",
      "Epoch: 025, Train Loss: 0.892, Train Acc: 0.776, Val Loss: 0.971, Val Acc: 0.714, Test Loss: 1.002, Test Acc: 0.688\n",
      "Epoch: 026, Train Loss: 0.892, Train Acc: 0.778, Val Loss: 0.971, Val Acc: 0.714, Test Loss: 1.003, Test Acc: 0.688\n",
      "Epoch: 027, Train Loss: 0.891, Train Acc: 0.776, Val Loss: 0.970, Val Acc: 0.714, Test Loss: 1.002, Test Acc: 0.688\n",
      "Epoch: 028, Train Loss: 0.891, Train Acc: 0.776, Val Loss: 0.971, Val Acc: 0.714, Test Loss: 1.001, Test Acc: 0.688\n",
      "Epoch: 029, Train Loss: 0.891, Train Acc: 0.776, Val Loss: 0.971, Val Acc: 0.714, Test Loss: 1.001, Test Acc: 0.688\n",
      "Epoch: 030, Train Loss: 0.891, Train Acc: 0.776, Val Loss: 0.970, Val Acc: 0.714, Test Loss: 1.002, Test Acc: 0.688\n",
      "Epoch: 031, Train Loss: 0.890, Train Acc: 0.776, Val Loss: 0.970, Val Acc: 0.714, Test Loss: 1.001, Test Acc: 0.688\n",
      "Epoch: 032, Train Loss: 0.890, Train Acc: 0.776, Val Loss: 0.970, Val Acc: 0.714, Test Loss: 1.002, Test Acc: 0.688\n",
      "Epoch: 033, Train Loss: 0.890, Train Acc: 0.778, Val Loss: 0.970, Val Acc: 0.723, Test Loss: 1.000, Test Acc: 0.688\n",
      "Epoch: 034, Train Loss: 0.890, Train Acc: 0.778, Val Loss: 0.970, Val Acc: 0.714, Test Loss: 1.001, Test Acc: 0.688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ivano\\Desktop\\DSMMaster\\CryptoThesis\\minculpool_example.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=33'>34</a>\u001b[0m patience \u001b[39m=\u001b[39m start_patience \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m150\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=35'>36</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=36'>37</a>\u001b[0m     _, train_acc \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=37'>38</a>\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m test(val_loader)\n",
      "\u001b[1;32mc:\\Users\\ivano\\Desktop\\DSMMaster\\CryptoThesis\\minculpool_example.ipynb Cell 4'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=7'>8</a>\u001b[0m out, mc_loss, o_loss \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49mbatch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(out, data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m+\u001b[39m mc_loss \u001b[39m+\u001b[39m o_loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000002?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ivano\\Desktop\\DSMMaster\\CryptoThesis\\minculpool_example.ipynb Cell 3'\u001b[0m in \u001b[0;36mNet.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000001?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, adj)\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000001?line=27'>28</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000001?line=29'>30</a>\u001b[0m x, adj, mc2, o2 \u001b[39m=\u001b[39m dense_mincut_pool(x, adj, s)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000001?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x, adj)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ivano/Desktop/DSMMaster/CryptoThesis/minculpool_example.ipynb#ch0000001?line=33'>34</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\dense\\mincut_pool.py:75\u001b[0m, in \u001b[0;36mdense_mincut_pool\u001b[1;34m(x, adj, s, mask)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch_geometric/nn/dense/mincut_pool.py?line=71'>72</a>\u001b[0m d_flat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mijk->ij\u001b[39m\u001b[39m'\u001b[39m, adj)\n\u001b[0;32m     <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch_geometric/nn/dense/mincut_pool.py?line=72'>73</a>\u001b[0m d \u001b[39m=\u001b[39m _rank3_diag(d_flat)\n\u001b[0;32m     <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch_geometric/nn/dense/mincut_pool.py?line=73'>74</a>\u001b[0m mincut_den \u001b[39m=\u001b[39m _rank3_trace(\n\u001b[1;32m---> <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch_geometric/nn/dense/mincut_pool.py?line=74'>75</a>\u001b[0m     torch\u001b[39m.\u001b[39mmatmul(torch\u001b[39m.\u001b[39;49mmatmul(s\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), d), s))\n\u001b[0;32m     <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch_geometric/nn/dense/mincut_pool.py?line=75'>76</a>\u001b[0m mincut_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(mincut_num \u001b[39m/\u001b[39m mincut_den)\n\u001b[0;32m     <a href='file:///c%3A/Users/ivano/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch_geometric/nn/dense/mincut_pool.py?line=76'>77</a>\u001b[0m mincut_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(mincut_loss)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(out, data.y.view(-1)) + mc_loss + o_loss\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(pred, data.y.view(-1)) + mc_loss + o_loss\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        correct += int(pred.max(dim=1)[1].eq(data.y.view(-1)).sum())\n",
    "\n",
    "    return loss_all / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = start_patience = 50\n",
    "for epoch in range(1, 150):\n",
    "    train_loss = train(epoch)\n",
    "    _, train_acc = test(train_loader)\n",
    "    val_loss, val_acc = test(val_loader)\n",
    "    if val_loss < best_val_loss:\n",
    "        test_loss, test_acc = test(test_loader)\n",
    "        best_val_acc = val_acc\n",
    "        patience = start_patience\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            break\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, '\n",
    "          f'Train Acc: {train_acc:.3f}, Val Loss: {val_loss:.3f}, '\n",
    "          f'Val Acc: {val_acc:.3f}, Test Loss: {test_loss:.3f}, '\n",
    "          f'Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c09dffd9390eb6dd651821c8eba1c4865319b367747d0473c0369ffb39ae3dcc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
